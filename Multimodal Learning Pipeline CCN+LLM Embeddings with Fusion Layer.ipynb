{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Business Goal\n",
    "\n",
    "1️⃣ Extract visual features from product images using CNN (EfficientNet-B7).\n",
    "\n",
    "2️⃣ Extract text embeddings from product descriptions using LLM (e.g., OpenAI GPT, Mistral, or Llama).\n",
    "\n",
    "3️⃣ Combine both embeddings for rich, context-aware predictions (e.g., shelf visibility, consumer interaction).\n",
    "\n",
    "4️⃣ Send embeddings to an LLM API to generate detailed insights about a product’s performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End-to-End Multimodal Learning Pipeline with Trainable Fusion Layer\n",
    "We will use a trainable fusion model to combine CNN-based image embeddings (EfficientNet-B7) and LLM-based text embeddings (OpenAI GPT, Mistral, or Llama). This will allow context-aware KPI predictions such as shelf visibility, consumer interaction, and buying patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Business Goal with Trainable Fusion Model\n",
    "\n",
    "✅ Step 1: Train CNN to Extract Image Embeddings\n",
    "\n",
    "We use EfficientNet-B7 to extract visual features from product images.\n",
    "\n",
    "These embeddings capture product placement, size, color, and shelf positioning.\n",
    "\n",
    "The embedding will be projected to a common 512-dimension space.\n",
    "\n",
    "✅ Step 2: Train LLM to Extract Text Embeddings\n",
    "\n",
    "We use Sentence Transformers (BERT, MiniLM, or OpenAI GPT-4 embeddings).\n",
    "\n",
    "The embeddings capture product name, description, and other metadata.\n",
    "\n",
    "The embedding will be projected to a 512-dimensional vector.\n",
    "\n",
    "✅ Step 3: \n",
    "Train a Fusion Model to Learn the Best Combination of Modalities\n",
    "\n",
    "We concatenate CNN and LLM embeddings and train a neural network to learn the optimal fusion strategy.\n",
    "\n",
    "The fusion model learns how visual & text embeddings relate to KPI predictions.\n",
    "\n",
    "✅ Step 4: Send the Final Embeddings to an LLM for Insights\n",
    "\n",
    "The trained fusion embedding is sent to an LLM (GPT-4, Mistral, or Claude-3).\n",
    "\n",
    "The LLM predicts KPIs, provides insights, and generates recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Train CNN to Extract Image Embeddings\n",
    "python\n",
    "Copy\n",
    "Edit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "\n",
    "# Load EfficientNet-B7\n",
    "efficientnet = models.efficientnet_b7(weights=models.EfficientNet_B7_Weights.DEFAULT)\n",
    "efficientnet.eval()\n",
    "\n",
    "# Define a transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Function to extract CNN embeddings\n",
    "def extract_cnn_embeddings(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = transform(image).unsqueeze(0)  # Convert to tensor\n",
    "    with torch.no_grad():\n",
    "        features = efficientnet.features(image)  # Extract CNN features\n",
    "        embedding = torch.flatten(features, start_dim=1)  # Flatten to a vector\n",
    "    return embedding  # Shape: (1, 2560)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Train LLM to Extract Text Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load Sentence Transformer model (alternative: OpenAI GPT API)\n",
    "text_model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # 384-dimension embeddings\n",
    "\n",
    "# Function to extract text embeddings\n",
    "def extract_text_embedding(text):\n",
    "    embedding = text_model.encode(text)\n",
    "    return torch.tensor(embedding).unsqueeze(0)  # Shape: (1, 384)\n",
    "\n",
    "# Example product description\n",
    "product_description = \"Shampoo on the second shelf in Walmart, easy to find.\"\n",
    "text_embedding = extract_text_embedding(product_description)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Train a Fusion Model to Learn Best Combination\n",
    "\n",
    "Reduce CNN embeddings from 2560 → 512 (using a projection layer).\n",
    "\n",
    "Expand Text embeddings from 384 → 512 (using a projection layer).\n",
    "\n",
    "Concatenate both embeddings and learn a fused representation.\n",
    "\n",
    "Train the fusion model using labeled KPI data (visibility, buying trends, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FusionModel, self).__init__()\n",
    "        self.cnn_projection = nn.Linear(2560, 512)  # Reduce CNN embedding size\n",
    "        self.text_projection = nn.Linear(384, 512)  # Expand text embedding size\n",
    "        self.fusion_layer = nn.Linear(1024, 512)  # Final fusion layer\n",
    "        self.classifier = nn.Linear(512, 5)  # Output KPI classes (e.g., visibility, findability)\n",
    "\n",
    "    def forward(self, cnn_emb, text_emb):\n",
    "        cnn_emb = self.cnn_projection(cnn_emb)  # Reduce CNN size\n",
    "        text_emb = self.text_projection(text_emb)  # Expand text size\n",
    "        combined = torch.cat((cnn_emb, text_emb), dim=1)  # Merge both embeddings\n",
    "        fused_embedding = self.fusion_layer(combined)  # Learn fusion representation\n",
    "        output = self.classifier(fused_embedding)  # Predict KPIs\n",
    "        return output\n",
    "\n",
    "# Initialize Model\n",
    "fusion_model = FusionModel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Train the Fusion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function & optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "fusion_model.to(device)\n",
    "optimizer = torch.optim.Adam(fusion_model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Simulated KPI labels (0 = low visibility, 4 = high findability)\n",
    "kpi_label = torch.tensor([3]).to(device)  # Example KPI class label\n",
    "\n",
    "# Training Function\n",
    "def train_fusion_model(image_path, text_desc, model, optimizer, criterion):\n",
    "    model.train()\n",
    "\n",
    "    # Extract embeddings\n",
    "    cnn_emb = extract_cnn_embeddings(image_path).to(device)\n",
    "    text_emb = extract_text_embedding(text_desc).to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    optimizer.zero_grad()\n",
    "    output = model(cnn_emb, text_emb)\n",
    "    loss = criterion(output, kpi_label)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Train for 10 epochs\n",
    "for epoch in range(10):\n",
    "    train_fusion_model(\"shelf.jpg\", product_description, fusion_model, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Send the Final Embedding to an LLM for Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "openai.api_key = \"your_openai_api_key\"\n",
    "\n",
    "def query_gpt(embedding):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert in retail shelf analysis.\"},\n",
    "            {\"role\": \"user\", \"content\": json.dumps({\n",
    "                \"embedding\": embedding.tolist(),\n",
    "                \"task\": \"Analyze shelf visibility, buying trends, and findability\"\n",
    "            })}\n",
    "        ]\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# Predict KPI for a product\n",
    "cnn_emb = extract_cnn_embeddings(\"shelf.jpg\").to(device)\n",
    "text_emb = extract_text_embedding(product_description).to(device)\n",
    "fused_embedding = fusion_model(cnn_emb, text_emb)\n",
    "\n",
    "# Send to GPT-4 for analysis\n",
    "analysis = query_gpt(fused_embedding.cpu().detach().numpy())\n",
    "print(\"LLM Response:\", analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Business Pipeline\n",
    "\n",
    "1️⃣ Extract CNN-based image embeddings (EfficientNet-B7).\n",
    "\n",
    "2️⃣ Extract LLM-based text embeddings (Sentence Transformer, GPT).\n",
    "\n",
    "3️⃣ Pass embeddings through a trainable fusion model (learn multimodal alignment).\n",
    "\n",
    "4️⃣ Send the fused embedding to GPT-4/Mistral for retail KPI analysis.\n",
    "\n",
    "Next--> Deploy this as an API (FastAPI, Flask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
