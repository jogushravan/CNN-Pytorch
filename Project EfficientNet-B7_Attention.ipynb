{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Business Goal\n",
    " Given an image of a retail shelf, we need to extract KPIs like: ✅ Visibility on Shelf → How well the product is placed?\n",
    "✅ Buying from Shelf → Are people interacting with it?\n",
    "✅ Findability on Shelf → How easy is it to locate?\n",
    "✅ Time to Find → How long does it take a customer to find the product?\n",
    "##### 1️⃣ Use YOLOv5 for Object Detection to detect products on the shelf.\n",
    "##### 2️⃣ Extract Attention Maps (Grad-CAM) from EfficientNet-B7 for feature enhancement.\n",
    "##### 3️⃣ Convert Attention Maps to Feature Embeddings.\n",
    "##### 4️⃣ Train a CNN Model (EfficientNet-B7) with embeddings to classify visibility, findability, etc.\n",
    "##### 5️⃣ Deploy Model for Real-Time KPI Estimation.\n",
    "\n",
    "Technical Approach\n",
    "\n",
    "1️⃣ Use YOLOv5 for Object Detection to detect products on the shelf.\n",
    "\n",
    "2️⃣ Extract Attention Maps (Grad-CAM) from EfficientNet-B7 for feature enhancement.\n",
    "\n",
    "3️⃣ Convert Attention Maps to Feature Embeddings.\n",
    "\n",
    "4️⃣ Train a CNN Model (EfficientNet-B7) with embeddings to classify visibility, findability, etc.\n",
    "\n",
    "5️⃣ Deploy Model for Real-Time KPI Estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Install Dependencies\n",
    "!pip install torch torchvision ultralytics opencv-python numpy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Detect Products on Shelf Using YOLOv5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRO TIP  Replace 'model=yolov5s.pt' with new 'model=yolov5su.pt'.\n",
      "YOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\n",
      "\n",
      "\n",
      "0: 640x352 (no detections), 127.4ms\n",
      "Speed: 3.1ms preprocess, 127.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 352)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolov5s.pt\")  # Load YOLOv5 for object detection\n",
    "model.eval()\n",
    "\n",
    "def detect_products(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    results = model(image)\n",
    "\n",
    "    bboxes = results[0].boxes.xyxy.cpu().numpy()  # Extract bounding boxes\n",
    "    class_ids = results[0].boxes.cls.cpu().numpy()  # Extract class labels\n",
    "\n",
    "    for bbox, cls in zip(bboxes, class_ids):\n",
    "        x1, y1, x2, y2 = map(int, bbox)\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Draw bounding box\n",
    "        cv2.putText(image, f\"Class {int(cls)}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Detected Products\", image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    return bboxes  # Return product locations\n",
    "\n",
    "image_path = \"Shavingrazor.jpg\"\n",
    "bboxes = detect_products(image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Extract Attention Features Using EfficientNet-B7\n",
    "extract attention maps from detected products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "efficientnet = models.efficientnet_b7(weights=models.EfficientNet_B7_Weights.DEFAULT)\n",
    "efficientnet.eval()\n",
    "target_layer = efficientnet.features[-1]  # Last conv layer for attention\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self.target_layer.register_forward_hook(self.save_activations)\n",
    "        self.target_layer.register_backward_hook(self.save_gradients)\n",
    "\n",
    "    def save_activations(self, module, input, output):\n",
    "        self.activations = output\n",
    "\n",
    "    def save_gradients(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0]\n",
    "\n",
    "    def generate(self, image_tensor, class_idx):\n",
    "        output = self.model(image_tensor)\n",
    "        loss = output[:, class_idx]\n",
    "        self.model.zero_grad()\n",
    "        loss.backward()\n",
    "        gradients = self.gradients.mean(dim=[2, 3], keepdim=True)\n",
    "        cam = (self.activations * gradients).sum(dim=1, keepdim=True)\n",
    "        cam = torch.relu(cam)\n",
    "        cam = cam.squeeze().cpu().detach().numpy()\n",
    "        cam = cv2.resize(cam, (224, 224))\n",
    "        cam = (cam - cam.min()) / (cam.max() - cam.min())\n",
    "        return cam\n",
    "\n",
    "def preprocess_product(image, bbox):\n",
    "    x1, y1, x2, y2 = map(int, bbox)\n",
    "    product = image[y1:y2, x1:x2]\n",
    "    product = cv2.resize(product, (224, 224))\n",
    "    product = product.astype(np.float32) / 255.0\n",
    "    product_tensor = torch.tensor(product).permute(2, 0, 1).unsqueeze(0)\n",
    "    return product_tensor\n",
    "\n",
    "def extract_attention_embeddings(image_path, bboxes):\n",
    "    image = cv2.imread(image_path)\n",
    "    embeddings = []\n",
    "    grad_cam = GradCAM(efficientnet, target_layer)\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        product_tensor = preprocess_product(image, bbox)\n",
    "        output = efficientnet(product_tensor)\n",
    "        class_idx = torch.argmax(output).item()\n",
    "        heatmap = grad_cam.generate(product_tensor, class_idx)\n",
    "        embedding = torch.tensor(heatmap).flatten().unsqueeze(0)  # Convert heatmap to 1D feature\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "    return torch.cat(embeddings, dim=0)  # Stack all embeddings\n",
    "\n",
    "attention_embeddings = extract_attention_embeddings(image_path, bboxes)\n",
    "print(\"Extracted attention embeddings shape:\", attention_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Train EfficientNet-B7 for KPI Classification\n",
    "\n",
    "Train EfficientNet-B7 using attention-enhanced embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KPIClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KPIClassifier, self).__init__()\n",
    "        self.base_model = models.efficientnet_b7(weights=models.EfficientNet_B7_Weights.DEFAULT)\n",
    "        self.base_model.classifier[1] = torch.nn.Linear(self.base_model.classifier[1].in_features, 256)\n",
    "        self.attention_fc = torch.nn.Linear(224 * 224, 256)\n",
    "        self.classifier = torch.nn.Linear(512, 5)  # 5 KPIs: Visibility, Findability, etc.\n",
    "\n",
    "    def forward(self, image_tensor, attention_embedding):\n",
    "        cnn_features = self.base_model(image_tensor)\n",
    "        attention_features = self.attention_fc(attention_embedding)\n",
    "        combined = torch.cat((cnn_features, attention_features), dim=1)\n",
    "        output = self.classifier(combined)\n",
    "        return output\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = KPIClassifier().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train_kpi_model(train_loader, model, optimizer, criterion, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for image_tensor, attention_embedding, kpi_label in train_loader:\n",
    "            image_tensor, attention_embedding, kpi_label = image_tensor.to(device), attention_embedding.to(device), kpi_label.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(image_tensor, attention_embedding)\n",
    "            loss = criterion(output, kpi_label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "train_kpi_model(train_loader, model, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Predict KPIs for New Shelf Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_kpis(image_path, model):\n",
    "    image = cv2.imread(image_path)\n",
    "    bboxes = detect_products(image_path)\n",
    "    attention_embedding = extract_attention_embeddings(image_path, bboxes)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        image_tensor = torch.randn(1, 3, 224, 224).to(device)  # Dummy tensor\n",
    "        attention_embedding = attention_embedding.to(device)\n",
    "        output = model(image_tensor, attention_embedding)\n",
    "        predictions = torch.argmax(output, dim=1)\n",
    "\n",
    "    return predictions.cpu().numpy()\n",
    "\n",
    "image_path = \"new_shelf.jpg\"\n",
    "print(f\"Predicted KPIs: {predict_kpis(image_path, model)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Business Impact\n",
    "\n",
    "✅ Retail Shelf Analysis → Measure how well a product is placed.\n",
    "\n",
    "✅ Consumer Behavior Tracking → Time to find and purchase probability.\n",
    "\n",
    "✅ Planogram Compliance → Ensuring correct product arrangement.\n",
    "\n",
    "✅ To deploy this as an API (FastAPI, Flask)?\n",
    "\n",
    "✅ To visualize the KPI trends using Power BI?\n",
    "\n",
    "## Deploying Retail Shelf KPI Detection as an API with Flask\n",
    "\n",
    "Step 1: Install Dependencies\n",
    "\n",
    "!pip install flask torch torchvision ultralytics opencv-python numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create a Flask App \n",
    "Create a new file app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import torchvision.models as models\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load YOLOv5 for product detection\n",
    "yolo_model = YOLO(\"yolov5s.pt\")\n",
    "yolo_model.eval()\n",
    "\n",
    "# Load EfficientNet-B7 for KPI classification\n",
    "efficientnet = models.efficientnet_b7(weights=models.EfficientNet_B7_Weights.DEFAULT)\n",
    "efficientnet.eval()\n",
    "target_layer = efficientnet.features[-1]\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        self.target_layer.register_forward_hook(self.save_activations)\n",
    "        self.target_layer.register_backward_hook(self.save_gradients)\n",
    "\n",
    "    def save_activations(self, module, input, output):\n",
    "        self.activations = output\n",
    "\n",
    "    def save_gradients(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0]\n",
    "\n",
    "    def generate(self, image_tensor, class_idx):\n",
    "        output = self.model(image_tensor)\n",
    "        loss = output[:, class_idx]\n",
    "        self.model.zero_grad()\n",
    "        loss.backward()\n",
    "        gradients = self.gradients.mean(dim=[2, 3], keepdim=True)\n",
    "        cam = (self.activations * gradients).sum(dim=1, keepdim=True)\n",
    "        cam = torch.relu(cam)\n",
    "        cam = cam.squeeze().cpu().detach().numpy()\n",
    "        cam = cv2.resize(cam, (224, 224))\n",
    "        cam = (cam - cam.min()) / (cam.max() - cam.min())\n",
    "        return cam\n",
    "\n",
    "def detect_products(image):\n",
    "    results = yolo_model(image)\n",
    "    bboxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "    return bboxes\n",
    "\n",
    "def extract_attention_embeddings(image, bboxes):\n",
    "    embeddings = []\n",
    "    grad_cam = GradCAM(efficientnet, target_layer)\n",
    "    \n",
    "    for bbox in bboxes:\n",
    "        x1, y1, x2, y2 = map(int, bbox)\n",
    "        product = image[y1:y2, x1:x2]\n",
    "        product = cv2.resize(product, (224, 224))\n",
    "        product = product.astype(np.float32) / 255.0\n",
    "        product_tensor = torch.tensor(product).permute(2, 0, 1).unsqueeze(0)\n",
    "        \n",
    "        output = efficientnet(product_tensor)\n",
    "        class_idx = torch.argmax(output).item()\n",
    "        heatmap = grad_cam.generate(product_tensor, class_idx)\n",
    "        embedding = torch.tensor(heatmap).flatten().unsqueeze(0)\n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    return torch.cat(embeddings, dim=0)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict_kpi():\n",
    "    if 'image' not in request.files:\n",
    "        return jsonify({\"error\": \"No image uploaded\"}), 400\n",
    "\n",
    "    file = request.files['image']\n",
    "    image = cv2.imdecode(np.frombuffer(file.read(), np.uint8), cv2.IMREAD_COLOR)\n",
    "    \n",
    "    bboxes = detect_products(image)\n",
    "    attention_embedding = extract_attention_embeddings(image, bboxes)\n",
    "\n",
    "    response = {\n",
    "        \"bboxes\": bboxes.tolist(),\n",
    "        \"attention_embedding_shape\": attention_embedding.shape[1]\n",
    "    }\n",
    "\n",
    "    return jsonify(response)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Run the Flask API\n",
    "\n",
    "python app.py\n",
    "\n",
    "test with Postman  or cURL\n",
    "curl -X POST -F \"image=@shelf.jpg\" http://127.0.0.1:5000/predict\n",
    "\n",
    "Expected Output\n",
    "\n",
    "{\n",
    "    \"bboxes\": [[100, 200, 250, 350], [300, 100, 450, 250]],\n",
    "    \"attention_embedding_shape\": 50176\n",
    "}\n",
    "\n",
    "\"bboxes\" → Detected product bounding boxes.\n",
    "\n",
    "\"attention_embedding_shape\" → Feature vector for each detected product."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda40_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
