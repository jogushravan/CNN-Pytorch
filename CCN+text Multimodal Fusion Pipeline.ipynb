{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multimodal Fusion Pipeline\n",
    "\n",
    "✅ Multimodal fusion enhances AI models by combining vision + text embeddings.\n",
    "\n",
    "✅ Self-attention refines features, and masking removes irrelevant data.\n",
    "\n",
    "✅ Pseudo-Patch Encoding ensures structured feature representation before classification.\n",
    "\n",
    "✅ EfficientNetV2 + LLM + CLIP + Self-Attention forms a powerful model pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps for Multimodal Fusion Model\n",
    "\n",
    "\n",
    "    1\tLoad dependencies\n",
    "    2\tApply data augmentation (images)\n",
    "    3\tExtract CNN image features (EfficientNetV2)\n",
    "    4\tExtract LLM text embeddings (BERT)\n",
    "    5\tCompute CLIP embeddings (align image + text)\n",
    "    6\tApply self-attention mechanism\n",
    "    7\tApply masking to filter irrelevant data\n",
    "    8\tConvert attention vectors into pseudo-patches\n",
    "    9\tPass features through a multimodal classifier\n",
    "    10\tTrain and optimize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Load Dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from transformers import AutoTokenizer, AutoModel, CLIPModel, CLIPProcessor\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Define Data Augmentation for Images\n",
    "transform = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Define Feature Extractors \n",
    "#EfficientNetV2 for Image Feature Extraction\n",
    "class EfficientNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        efficientnet = models.efficientnet_v2_l(weights=\"IMAGENET1K_V1\")\n",
    "        self.feature_extractor = nn.Sequential(*list(efficientnet.children())[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.feature_extractor(x).squeeze()\n",
    "\n",
    "#LLM for Text Feature Extraction (BERT)\n",
    "class TextEmbeddingLLM(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, text):\n",
    "        tokens = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        embeddings = self.model(**tokens).last_hidden_state.mean(dim=1)\n",
    "        return embeddings\n",
    "##CLIP Model for Image-Text Alignment\n",
    "class CLIPFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        inputs = self.processor(text=text, images=image, return_tensors=\"pt\", padding=True)\n",
    "        outputs = self.model(**inputs)\n",
    "        return outputs.text_embeds, outputs.image_embeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Step 4: Define Self-Attention & Masking Mechanism\n",
    "#Self-Attention Layer\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q, K, V = self.query(x), self.key(x), self.value(x)\n",
    "        attention_scores = self.softmax(Q @ K.transpose(-2, -1) / (embed_dim ** 0.5))\n",
    "        return attention_scores @ V\n",
    "#Masking Mechanism\n",
    "def apply_masking(attention_scores, mask):\n",
    "    if mask is not None:\n",
    "        attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "    return F.softmax(attention_scores, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Define Pseudo-Patch Encoding\n",
    "class PseudoPatchEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, patch_size=16):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.projection = nn.Linear(input_dim, input_dim // patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.projection(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Multimodal Classifier\n",
    "class MultimodalClassifier(nn.Module):\n",
    "    def __init__(self, image_dim=1280, text_dim=768, embed_dim=512, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.image_fc = nn.Linear(image_dim, embed_dim)\n",
    "        self.text_fc = nn.Linear(text_dim, embed_dim)\n",
    "        self.self_attention = SelfAttention(embed_dim)\n",
    "        self.pseudo_patch_encoder = PseudoPatchEncoder(embed_dim)\n",
    "        self.classifier = nn.Linear(embed_dim // 16, num_classes)\n",
    "\n",
    "    def forward(self, image_feats, text_feats):\n",
    "        img_embed = self.image_fc(image_feats)\n",
    "        txt_embed = self.text_fc(text_feats)\n",
    "        combined = torch.cat([img_embed, txt_embed], dim=-1)\n",
    "        attn_output = self.self_attention(combined)\n",
    "        patches = self.pseudo_patch_encoder(attn_output)\n",
    "        return self.classifier(patches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Define Dataset Loader\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, image_paths, text_data, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.text_data = text_data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        text = self.text_data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image=np.array(image))[\"image\"]\n",
    "\n",
    "        return image, text, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Training the Model\n",
    "# Initialize Models\n",
    "image_model = EfficientNetFeatureExtractor()\n",
    "text_model = TextEmbeddingLLM()\n",
    "clip_model = CLIPFeatureExtractor()\n",
    "multimodal_model = MultimodalClassifier()\n",
    "\n",
    "# Optimizer and Loss\n",
    "optimizer = torch.optim.Adam(multimodal_model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(MultimodalDataset([\"image1.jpg\", \"image2.jpg\"], \n",
    "                                            [\"This is a product\", \"Another product\"], \n",
    "                                            [0, 1], transform=transform), batch_size=2, shuffle=True)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(10):\n",
    "    for images, texts, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Extract Features\n",
    "        image_feats = image_model(images)\n",
    "        text_feats = text_model(texts)\n",
    "\n",
    "        # Apply CLIP\n",
    "        text_clip_feats, image_clip_feats = clip_model(images, texts)\n",
    "\n",
    "        # Combine Features\n",
    "        combined_image_feats = image_feats + image_clip_feats\n",
    "        combined_text_feats = text_feats + text_clip_feats\n",
    "\n",
    "        # Forward Pass\n",
    "        outputs = multimodal_model(combined_image_feats, combined_text_feats)\n",
    "\n",
    "        # Compute Loss & Optimize\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda40_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
